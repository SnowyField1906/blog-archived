---
title: To√†n t·∫≠p v·ªÅ b√†i to√°n Least Squares v√† c√°c ph∆∞∆°ng ph√°p gi·∫£i
date: '2023-10-16'
tags:
  [
    'Supervised Learning',
    'Machine Learning',
    'Optimization',
    'Linear Algebra',
    'Calculus',
    'Mathematics',
  ]
draft: false
summary: Gi·ªõi thi·ªáu chi ti·∫øt v·ªÅ ph∆∞∆°ng ph√°p Least Squares v√† c√°ch gi·∫£i b√†i to√°n Least Squares b·∫±ng c√°c ph∆∞∆°ng ph√°p kh√°c nhau.
layout: PostView
thumbnail: '/static/images/thumbnails/toan-tap-ve-bai-toan-least-squares-va-cac-phuong-phap-giai.png'
---

_H·ªìi quy l√† m·ªôt b√†i to√°n kinh ƒëi·ªÉn trong Machine Learning m√† Least Squares (B√¨nh ph∆∞∆°ng T·ªëi ti·ªÉu) l√† m·ªôt trong nh·ªØng ph∆∞∆°ng ph√°p ph·ªï bi·∫øn. B·∫±ng c√°ch x√¢y d·ª±ng m·ªôt model t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn ƒë·∫ßu v√†o v√† bi·∫øn m·ª•c ti√™u t·ª´ d·ªØ li·ªáu hu·∫•n luy·ªán, ch√∫ng ta c√≥ th·ªÉ d·ª± ƒëo√°n gi√° tr·ªã m·ª•c ti√™u cho c√°c d·ªØ li·ªáu m·ªõi._

_Trong b√†i vi·∫øt n√†y, ch√∫ng ta s·∫Ω t√¨m hi·ªÉu v·ªÅ Least Squares v√† c√°c ph∆∞∆°ng ph√°p gi·∫£i c·ª• th·ªÉ v√† chi ti·∫øt cho b√†i to√°n n√†y._

<img className="w-full flex justify-center mx-auto" src="/static/images/thumbnails/toan-tap-ve-bai-toan-least-squares-va-cac-phuong-phap-giai.png" alt="To√†n t·∫≠p v·ªÅ b√†i to√°n Least Squares v√† c√°c ph∆∞∆°ng ph√°p gi·∫£i" />

> Khuy·∫øn ngh·ªã ƒë·ªçc tr∆∞·ªõc [QR Decomposition l√† g√¨ v√† chi ti·∫øt c√°ch t√≠nh](https://snowyfield.software/posts/qr-decomposition-la-gi-va-chi-tiet-cach-tinh) v√† [Singular Value Decomposition l√† g√¨ v√† chi ti·∫øt c√°ch t√≠nh](https://snowyfield.software/posts/singular-value-decomposition-la-gi-va-chi-tiet-cach-tinh) ƒë·ªÉ s·∫µn s√†ng tr∆∞·ªõc khi ƒëi v√†o b√†i vi·∫øt n√†y.

## B√†i to√°n Least Squares

### Nh·∫Øc l·∫°i

#### Model

**Model** (M√¥ h√¨nh) m√† ch√∫ng ta hay nghe qua th·ª±c ch·∫•t l√† m·ªôt h√†m s·ªë $f(x, \theta)$ v·ªõi $x$ l√† m·ªôt t·∫≠p h·ª£p c√°c **input** (ƒë·∫ßu v√†o) v√† $\theta$ l√† m·ªôt t·∫≠p h·ª£p c√°c **parameter** (tham s·ªë).

V·ªõi m·ªói gi√° tr·ªã $x$, model s·∫Ω cho ra m·ªôt gi√° tr·ªã $y$ l√† **output** (ƒë·∫ßu ra) t∆∞∆°ng ·ª©ng. $y$ th·ªÉ l√† m·ªôt c√¢u tr·∫£ l·ªùi cho m·ªôt c√¢u h·ªèi n√†o ƒë√≥, m·ªôt b·ª©c ·∫£nh ƒë∆∞·ª£c t·∫°o ra hay m·ªôt d·ª± ƒëo√°n v·ªÅ m·ªôt s·ª± ki·ªán trong t∆∞∆°ng lai.

#### Ma tr·∫≠n

**Ma tr·∫≠n** l√† m·ªôt t·∫≠p h·ª£p c√°c s·ªë ƒë∆∞·ª£c s·∫Øp x·∫øp th√†nh c√°c h√†ng v√† c·ªôt. N√≥ gi√∫p bi·ªÉu di·ªÖn c√°c chi·ªÅu kh√¥ng gian cao h∆°n.

√ù nghƒ©a h√¨nh h·ªçc c·ªßa ma tr·∫≠n l√† n√≥ bi·ªÉu di·ªÖn m·ªôt ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh, hay n√≥i c√°ch kh√°c n√≥ gi·ªëng nh∆∞ m·ªôt h√†m s·ªë bi·∫øn ƒë·ªïi m·ªôt kh√¥ng gian n√†y sang m·ªôt kh√¥ng gian kh√°c m√† ph√©p bi·∫øn ƒë·ªïi n√†y c√≥ th·ªÉ l√† t·ªï h·ª£p c·ªßa c√°c ph√©p **rotate** (quay), **stretch** (co gi√£n), **tr∆∞·ª£t** (shear),...

Ma tr·∫≠n $\mathbf{A} \in \mathbb{R}^{m \times n}$ bi·ªÉu di·ªÖn m·ªôt ph√©p bi·∫øn ƒë·ªïi t·ª´ kh√¥ng gian $\mathbb{R}^n$ sang kh√¥ng gian $\mathbb{R}^m$.

#### Least Squares

**Least Squares** (B√¨nh ph∆∞∆°ng T·ªëi ti·ªÉu) l√† m·ªôt ph∆∞∆°ng ph√°p t·ªëi ∆∞u h√≥a ƒë·ªÉ l·ª±a ch·ªçn m·ªôt ƒë∆∞·ªùng kh·ªõp nh·∫•t cho m·ªôt t·∫≠p h·ª£p c√°c ƒëi·ªÉm d·ªØ li·ªáu. ƒê∆∞·ªùng kh·ªõp nh·∫•t l√† ƒë∆∞·ªùng c√≥ t·ªïng b√¨nh ph∆∞∆°ng sai s·ªë nh·ªè nh·∫•t gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu v√† ƒë∆∞·ªùng ƒë√≥.

Nghi·ªám c·ªßa b√†i to√°n Least Squares l√† t·∫≠p h·ª£p gi√° tr·ªã c·ªßa c√°c **parameter** ·ª©ng v·ªõi h√†m s·ªë ƒë√£ ch·ªçn. T·ª´ ƒë√≥ ta s·∫Ω c√≥ ƒë∆∞·ª£c m·ªôt model c√≥ th·ªÉ d·ª± ƒëo√°n ƒë∆∞·ª£c gi√° tr·ªã **output** cho m·ªôt **input** b·∫•t k√¨.

### C√¥ng th·ª©c

G·ªçi s·ª± ch√™nh l·ªách gi·ªØa gi√° tr·ªã quan s√°t ƒë∆∞·ª£c $y$ v√† gi√° tr·ªã m√† model d·ª± ƒëo√°n $\hat{y}$ l√† $e$.

$$
\begin{align*}
e &= \hat{y} - y \\
&= f(x, \theta) - y \tag{1}
\end{align*}
$$

Ta c√≥ th·ªÉ t√≠nh ƒë∆∞·ª£c t·ªïng b√¨nh ph∆∞∆°ng sai s·ªë $S$:

$$
\begin{align*}
S &= \sum_{i=1}^{n} (\hat{y_i} - y_i) \\
\end{align*}
$$

Tuy nhi√™n $e$ c√≥ th·ªÉ nh·∫≠n gi√° tr·ªã √¢m, d·∫´n ƒë·∫øn vi·ªác kh√¥ng th·ªÉ so s√°nh ƒë∆∞·ª£c v·ªõi c√°c gi√° tr·ªã kh√°c. V√¨ v·∫≠y ch√∫ng ta s·∫Ω n√¢ng c·∫•p h√†m $S$ nh∆∞ sau:

$$
\begin{aligned}
S &= \sum_{i=1}^{n} |\hat{y_i} - y_i|
\end{aligned}
$$

L√∫c n√†y $S$ ƒë√£ ph·∫£n √°nh ƒë√∫ng ch·ª©c nƒÉng c·ªßa vi·ªác m√¥ ph·ªèng ƒë·ªô l·ªách gi·ªØa 2 t·∫≠p gi√° tr·ªã, tuy nhi√™n h√†m tr·ªã tuy·ªát ƒë·ªëi $|x|$ l√† m·ªôt h√†m kh√¥ng kh·∫£ vi t·∫°i $x = 0$, n√™n ch√∫ng ta kh√¥ng th·ªÉ d√πng n√≥ ƒë·ªÉ t√≠nh ƒë·∫°o h√†m / gradient. V√¨ v·∫≠y ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng h√†m b√¨nh ph∆∞∆°ng ƒë·ªÉ thay th·∫ø cho h√†m tr·ªã tuy·ªát ƒë·ªëi:

$$
\begin{aligned}
L = \frac{1}{2}(\hat{y} - y)^2 \tag{2}
\end{aligned}
$$

L√≠ do $\frac{1}{2}$ ƒë∆∞·ª£c th√™m v√†o l√† ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a vi·ªác t√≠nh ƒë·∫°o h√†m / gradient c·ªßa $L$: $\nabla L = \hat{y} - y$.

Sau khi t√¨m ƒë∆∞·ª£c nghi·ªám, ch√∫ng ta c√≥ th·ªÉ quan s√°t ƒë∆∞·ª£c v·ªã tr√≠ t∆∞∆°ng quan gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu v√† model.

<figure>
<img className="w-full md:w-1/2 flex justify-center mx-auto" alt="ƒê∆∞·ªùng h·ªìi quy" src="/static/images/posts/sum-of-squares.png" />
<figcaption className="text-center text-gray-500">Source: ProProcessEngineer (YouTube)</figcaption>
</figure>

### L·ªùi gi·∫£i

Cho $f(x, \theta)$ l√† m·ªôt model c√≥ $m$ tham s·ªë: $\theta = \{\theta_0, \theta_1, \dots, \theta_m\}$.

ƒê·ªÉ $S$ ƒë·∫°t gi√° tr·ªã nh·ªè nh·∫•t, ta s·∫Ω ph·∫£i t√¨m c√°c parameter $\theta$ sao cho $S$ ƒë·∫°t c·ª±c tr·ªã, v√¨ c·ª±c tr·ªã duy nh·∫•t c·ªßa $S$ s·∫Ω lu√¥n r∆°i v√†o tr∆∞·ªùng h·ª£p c·ª±c ti·ªÉu do ƒë√¢y l√† t·ªïng c·ªßa c√°c $r^2 \geq 0$.

L√∫c n√†y, ta ch·ªâ c·∫ßn ƒë·∫∑t gradient c·ªßa $S$ v·ªÅ $0$, hay ƒë·∫∑t ƒë·∫°o h√†m c·ªßa $S$ v·ªÅ $0$ theo t·ª´ng parameter $\theta$, v√¨ l√∫c n√†y $\theta$ c√≥ vai tr√≤ nh∆∞ l√† m·ªôt bi·∫øn/·∫©n.

$$
\begin{align*}
\nabla S(\theta) &= \vec{0} \\
\frac{\partial S}{\partial \theta_j} &= 0 \quad \text{v·ªõi} \quad j = 0, 1, \dots, m \\
\frac{\partial}{\partial \theta_j} \left( \frac{1}{2} \sum_{i=1}^{n} r_i^2 \right) &= 0 \\
\sum_{i=1}^{n} r_i \frac{\partial r_i}{\partial \theta_j} &= 0 \\
\sum_{i=1}^{n} (f(x_i, \theta) - y_i) \frac{\partial (f(x_i, \theta) - y_i)}{\partial \theta_j} &= 0 \\
\sum_{i=1}^{n} (f(x_i, \theta) - y_i) \frac{\partial f(x_i, \theta)}{\partial \theta_j} &= 0 \tag{3} \\
\end{align*}
$$

Ph∆∞∆°ng tr√¨nh tr√™n √°p d·ª•ng cho m·ªçi b√†i to√°n B√¨nh ph∆∞∆°ng T·ªëi ti·ªÉu.

### V√≠ d·ª•

Cho t·∫≠p d·ªØ li·ªáu $(x, y)$ c√≥ ƒë·ªô l·ªõn $n = 10$:

$$
\begin{array}{c|c|c|c|c|c|c|c|c|c|c}
x & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
\hline
y & 1 & 3 & 2 & 5 & 7 & 8 & 8 & 9 & 10 & 12 \\
\end{array} \tag{4}
$$

#### M√¥ h√¨nh $y = \theta_0 + \theta_1 x$

T·ª´ $(3)$, ta c√≥ h·ªá ph∆∞∆°ng tr√¨nh:

$$
\begin{align*}
& \begin{cases}
\sum (\theta_0 + \theta_1 x_i - y_i) \frac{\partial f(x_i, \theta)}{\partial \theta_0} &= 0 \\
\sum (\theta_0 + \theta_1 x_i - y_i) \frac{\partial f(x_i, \theta)}{\partial \theta_1} &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
\sum (\theta_0 + \theta_1 x_i - y_i) &= 0 \\
\sum (\theta_0 + \theta_1 x_i - y_i) x_i &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
\sum \theta_0 + \theta_1 \sum x_i - \sum y_i &= 0 \\
\sum \theta_0 x_i + \theta_1 \sum x_i^2 - \sum x_i y_i &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
n \theta_0 + \theta_1 \sum x_i - \sum y_i &= 0 \\
\theta_0 \sum x_i + \theta_1 \sum x_i^2 - \sum x_i y_i &= 0 \tag{5} \\
\end{cases} \\
\end{align*}
$$

T·ª´ $(4)$, ta c√≥:

$$
\begin{array}{c|c|c|c|c}
t & x & y & x^2 & xy \\
\hline
\sum{t} & 55 & 65 & 385 & 454 \\
\end{array}
$$

Thay v√†o $(5)$, ta ƒë∆∞·ª£c:

$$
\begin{align*}
& \begin{cases}
10 \theta_0 + 55 \theta_1 - 67 &= 0 \\
55 \theta_0 + 385 \theta_1 - 550 &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
\theta_0 &\approx 0.07 \\
\theta_1 &\approx 1.17 \\
\end{cases} \\
\end{align*}
$$

<figure>
<img className="w-full md:w-1/2 flex justify-center mx-auto" alt="ƒê∆∞·ªùng h·ªìi quy" src="/static/images/posts/assignment-1-chart.png" />
<figcaption className="text-center text-gray-500">Nghi·ªám c·ªßa b√†i to√°n l√† $y = 0.07 + 1.17x$</figcaption>
</figure>

#### M√¥ h√¨nh $y = \theta_0 + \theta_1 e^x + \theta_2 x^2$

T·ª´ $(3)$, ta c√≥ h·ªá ph∆∞∆°ng tr√¨nh:

$$
\begin{align*}
& \begin{cases}
\sum (\theta_0 + \theta_1 e^{x_i} + \theta_2 x_i^2 - y_i) \frac{\partial f(x_i, \theta)}{\partial \theta_0} &= 0 \\
\sum (\theta_0 + \theta_1 e^{x_i} + \theta_2 x_i^2 - y_i) \frac{\partial f(x_i, \theta)}{\partial \theta_1} &= 0 \\
\sum (\theta_0 + \theta_1 e^{x_i} + \theta_2 x_i^2 - y_i) \frac{\partial f(x_i, \theta)}{\partial \theta_2} &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
\sum (\theta_0 + \theta_1 e^{x_i} + \theta_2 x_i^2 - y_i) &= 0 \\
\sum (\theta_0 + \theta_1 e^{x_i} + \theta_2 x_i^2 - y_i) e^{x_i} &= 0 \\
\sum (\theta_0 + \theta_1 e^{x_i} + \theta_2 x_i^2 - y_i) x_i^2 &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
\sum \theta_0 + \theta_1 \sum e^{x_i} + \theta_2 \sum x_i^2 - \sum y_i &= 0 \\
\sum \theta_0 e^{x_i} + \theta_1 \sum e^{2x_i} + \theta_2 \sum e^{x_i} x_i^2 - \sum e^{x_i} y_i &= 0 \\
\sum \theta_0 x_i^2 + \theta_1 \sum e^{x_i} x_i^2 + \theta_2 \sum x_i^4 - \sum x_i^2 y_i &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
n \theta_0 + \theta_1 \sum e^{x_i} + \theta_2 \sum x_i^2 - \sum y_i &= 0 \\
\theta_0 \sum e^{x_i} + \theta_1 \sum e^{2x_i} + \theta_2 \sum e^{x_i} x_i^2 - \sum e^{x_i} y_i &= 0 \\
\theta_0 \sum x_i^2 + \theta_1 \sum e^{x_i} x_i^2 + \theta_2 \sum x_i^4 - \sum x_i^2 y_i &= 0 \tag{6} \\
\end{cases} \\
\end{align*}
$$

T·ª´ $(4)$, ta c√≥:

$$
\begin{array}{c|c|c|c|c|c|c|c|c}
t & y & x^2 & x^2 y & e^x & e^x y & e^{2x} & e^{x} x^2 & x^4 \\
\hline
\sum{t} & 65 & 385 & 3552 &34843,77 & 385554,49 & 561102107 & 920186,51 & 25333
\end{array}
$$

Thay v√†o $(6)$, ta ƒë∆∞·ª£c:

$$
\begin{align*}
& \begin{cases}
10 \theta_0 + 34843,77 \theta_1 + 25333 \theta_2 - 65 &= 0 \\
34843,77 \theta_0 + 561102107 \theta_1 + 920186,51 \theta_2 - 385554,49 &= 0 \\
25333 \theta_0 + 920186,51 \theta_1 + 25333 \theta_2 - 3552 &= 0 \\
\end{cases} \\
\implies
& \begin{cases}
\theta_0 &\approx 0.1140 \\
\theta_1 &\approx 0.0007 \\
\theta_2 &\approx 0.0016 \\
\end{cases} \\
\end{align*}
$$

### M·ªü r·ªông

Ta c√≥ th·ªÉ ƒë∆∞a b√†i to√°n v·ªÅ d·∫°ng ma tr·∫≠n mi·ªÖn l√† model c√≥ d·∫°ng ƒëa th·ª©c nh∆∞ sau:

$$
\begin{align*}
y &= \sum_{i=1}^{n} a_i \phi_i(x) \\
y &= a_1 \phi_1(x) + a_2 \phi_2(x) + \dots + a_n \phi_n(x) \\
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} &= \begin{bmatrix} \phi_1(x_1) & \phi_2(x_1) & \dots & \phi_n(x_1) \\ \phi_1(x_2) & \phi_2(x_2) & \dots & \phi_n(x_2) \\ \vdots & \vdots & \ddots & \vdots \\ \phi_1(x_m) & \phi_2(x_m) & \dots & \phi_n(x_m) \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} \\
\mathbf{y} &= \mathbf{A} \mathbf{x} \tag{7} \\
\end{align*}
$$

Trong ƒë√≥, $\mathbf{A} \in \mathbb{R}^{m \times n}$ l√† m·ªôt ma tr·∫≠n ch·ª©a gi√° tr·ªã c·ªßa c√°c **Basic Function** (H√†m s·ªë C∆° b·∫£n) $\phi$ t·∫°i c√°c ƒëi·ªÉm $x$ ƒë√£ cho.

> **üìù Nh·∫Øc l·∫°i**
>
> **Basic Function** (H√†m s·ªë C∆° b·∫£n) l√† m·ªôt h√†m s·ªë m√† c√°c h√†m s·ªë kh√°c c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng t·ªïng tuy·∫øn t√≠nh c·ªßa n√≥. C√≥ nghƒ©a l√† ma tr·∫≠n $\mathbf{A}$ c√≥ th·ªÉ bi·ªÉu di·ªÖn ƒë∆∞·ª£c.
>
> V√≠ d·ª•: $y = \theta_0 sin(x) + \theta_1 cos(x)$ c√≥ th·ªÉ bi·ªÉu di·ªÖn ƒë∆∞·ª£c b·∫±ng ma tr·∫≠n $\mathbf{A} = \begin{bmatrix} sin(x_i) & cos(x_i) \end{bmatrix}$ v·ªõi c√°c basic function l√† $sin(x)$ v√† $cos(x)$. Trong khi ƒë√≥ $y = sin(\theta_0 x) + cos(\theta_1 x)$ kh√¥ng th·ªÉ bi·ªÉu di·ªÖn ƒë∆∞·ª£c v√¨ c√°c parameter $\theta_0$ v√† $\theta_1$ n·∫±m trong h√†m s·ªë.

Cho tr∆∞·ªõc $\mathbf{b} \in \mathbb{R}^m$ l√† m·ªôt vector ch·ª©a c√°c gi√° tr·ªã $y$ quan s√°t ƒë∆∞·ª£c, l√∫c n√†y nhi·ªám v·ª• c·ªßa ch√∫ng ta l√† x·∫•p x·ªâ nghi·ªám c·ªßa $\mathbf{y}$ theo $\mathbf{b}$ v·ªõi sai s·ªë nh·ªè nh·∫•t:

$$
\begin{align*}
\mathbf{y} &\approx \mathbf{b} \\
\mathbf{A} \mathbf{x} &\approx \mathbf{b} \tag{8} \\
\end{align*}
$$

L√∫c n√†y b√†i to√°n tr·ªü th√†nh t√¨m vector $\mathbf{x} \in \mathbb{R}^n$ sao cho:

$$
\begin{align*}
&\min_{x} ||\mathbf{y} - \mathbf{b}||_2 \\
= &\min_{x} ||\mathbf{A}\mathbf{x} - \mathbf{b}||_2 \tag{9}
\end{align*}
$$

T·∫≠p nghi·ªám c·ªßa b√†i to√°n n√†y l√†:

$$
\begin{align*}
\mathbf{A} \mathbf{x} &= \mathbf{b} \\
\mathbf{A}^T \mathbf{A} \mathbf{x} &= \mathbf{A}^T \mathbf{b} \\
\mathbf{x} &= (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b} \\
\mathbf{x} &= \mathbf{A}^\dagger \mathbf{b} \tag{10} \\
\end{align*}
$$

> **üìù Nh·∫Øc l·∫°i**
>
> $\mathbf{A}$ l√† **Non-singular Matrix** (Ma tr·∫≠n Kh·∫£ ngh·ªãch) khi v√† ch·ªâ khi $\mathbf{A}$ l√† **ma tr·∫≠n vu√¥ng** v√† **full rank** (ƒë·ªß h·∫°ng). V√¨ v·∫≠y, trong nhi·ªÅu tr∆∞·ªùng h·ª£p, ch√∫ng ta c√≥ th·ªÉ thay th·∫ø $\mathbf{A}^{-1}$ b·∫±ng $\mathbf{A}^\dagger$. ƒê√¢y l√† m·ªôt **Pseudo Inverse Matrix** (Ma tr·∫≠n Gi·∫£ Ngh·ªãch) ƒë∆∞·ª£c t√≠nh b·∫±ng $(\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T$, v√¨ $\mathbf{A}^T\mathbf{A}$ lu√¥n kh·∫£ ngh·ªãch.
>
> √ù nghƒ©a h√¨nh h·ªçc c·ªßa **Inverse Matrix** (Ma tr·∫≠n Ngh·ªãch ƒë·∫£o) l√† n·∫øu $\mathbf{A}$ bi·∫øn ƒë·ªïi m·ªôt kh√¥ng gian n√†o ƒë√≥ th√¨ $\mathbf{A}^{-1}$ s·∫Ω bi·∫øn ƒë·ªïi l·∫°i v·ªÅ kh√¥ng gian ban ƒë·∫ßu. Trong khi ƒë√≥ **Pseudo Inverse Matrix** s·∫Ω bi·∫øn ƒë·ªïi v·ªÅ m·ªôt kh√¥ng gian v·ªõi c√°c chi·ªÅu g·∫ßn gi·ªëng kh√¥ng gian ban ƒë·∫ßu nh·∫•t.

Trong tr∆∞·ªùng h·ª£p l√≠ t∆∞·ªüng, khi m√† ma tr·∫≠n $\mathbf{A}$ kh·∫£ ngh·ªãch (full rank v√† vu√¥ng) th√¨ ƒë·ªìng nghƒ©a v·ªõi vi·ªác ph∆∞∆°ng tr√¨nh $\mathbf{A} \mathbf{x} = \mathbf{b}$ c√≥ nghi·ªám duy nh·∫•t. ƒê·ªìng nghƒ©a v·ªõi vi·ªác t·ªìn t·∫°i m·ªôt ƒë∆∞·ªùng ƒëi qua t·∫•t c·∫£ c√°c ƒëi·ªÉm ƒë√£ cho.

T·∫•t nhi√™n, trong m·ªçi tr∆∞·ªùng h·ª£p th√¨ h·∫ßu h·∫øt th√¨ ph∆∞∆°ng tr√¨nh tr√™n s·∫Ω v√¥ nghi·ªám, hay ma tr·∫≠n $\mathbf{A}$ s·∫Ω kh√¥ng kh·∫£ ngh·ªãch. Do ƒë√≥ ch√∫ng ta nh·ªù ƒë·∫øn **Pseudo Inverse Matrix** v√¨ n√≥ s·∫Ω gi√∫p ta x·∫•p x·ªâ ƒë∆∞·ª£c nghi·ªám l√≠ t∆∞·ªüng cho b√†i to√°n.

### V√≠ d·ª•

S·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu ·ªü $(4)$ ƒë·ªÉ x·∫•p x·ªâ ƒë∆∞·ªùng th·∫≥ng $y = \theta_0 + \theta_1 x + \theta_2 x^2$.

$$
\begin{align*}
\mathbf{A} &= \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ \vdots & \vdots & \vdots \\ 1 & 10 & 100 \end{bmatrix} \quad \mathbf{b} = \begin{bmatrix} 1 \\ 3 \\ \vdots \\ 12 \end{bmatrix} \\
\implies \mathbf{A}^T \mathbf{A} &= \begin{bmatrix} 10 & 55 & 385 \\ 55 & 385 & 3025 \\ 385 & 3025 & 25333 \end{bmatrix} \\
\implies (\mathbf{A}^T \mathbf{A})^{-1} &= \begin{bmatrix} 1.38 & -0.526 & 0.0417 \\ -0.525 & 0.241 & -0.0208 \\ 0.0417 & -0.0208 & 0.00189 \end{bmatrix} \\
\implies (\mathbf{A}^T \mathbf{A})^{-1}\mathbf{A}^T &= \begin{bmatrix}
0,9 & 0,5 & 0,18 & -0,05 & -0,2 & -0,27 & -0,25 & -0,15 & 0,03 & 0,3 \\
-0,3 & -0,13 & 0,01 & 0,1 & 0,16 & 0,17 & 0,14 & 0,07 & -0,04 & -0,2 \\
0,02 & 0,008 & -0,004 & -0,011 & -0,015 & -0,015 & -0,011 & -0,004 & 0,008 & 0,023
\end{bmatrix} \\
\implies (\mathbf{A}^T \mathbf{A})^{-1}\mathbf{A}^T\mathbf{b} &= \begin{bmatrix} -0.43 \\ 1.42 \\ -0.02 \end{bmatrix} \\
\end{align*}
$$

<figure>
<img className="w-full md:w-1/2 flex justify-center mx-auto" alt="ƒê∆∞·ªùng h·ªìi quy" src="/static/images/posts/assignment-3-chart.png" />
<figcaption className="text-center text-gray-500">Nghi·ªám c·ªßa b√†i to√°n l√† $y = -0.43 + 1.42x - 0.02x^2$</figcaption>
</figure>

## QR Decomposition

**QR Decomposition** (Ph√¢n r√£ QR) l√† m·ªôt ph∆∞∆°ng ph√°p ph√¢n r√£ m·ªôt ma tr·∫≠n b·∫•t k√¨ th√†nh t√≠ch c·ªßa hai ma tr·∫≠n $\mathbf{Q}$ v√† $\mathbf{R}$, trong ƒë√≥ $\mathbf{Q}$ l√† m·ªôt **Orthonormal Matrix** (Ma tr·∫≠n Tr·ª±c chu·∫©n) v√† $\mathbf{R}$ l√† m·ªôt **Upper Triangular Matrix** (Ma tr·∫≠n Tam gi√°c tr√™n).

### Gi·∫£i b√†i to√°n Least Squares b·∫±ng QR Decomposition

M·ªôt khi th·ªÉ ph√¢n r√£ $\mathbf{A}$ th√†nh t√≠ch c·ªßa hai ma tr·∫≠n $\mathbf{Q}$ v√† $\mathbf{R}$, thay v√¨ d√πng ma tr·∫≠n gi·∫£ ngh·ªãch nh∆∞ $(9)$, ta c√≥ th·ªÉ bi·∫øn ƒë·ªïi th√†nh:

$$
\begin{align*}
\mathbf{A} \mathbf{x} &= \mathbf{b} \\
\mathbf{Q} \mathbf{R} \mathbf{x} &= \mathbf{b} \\
\mathbf{R} \mathbf{x} &= \mathbf{Q}^T \mathbf{b} \\
\mathbf{x} &= \mathbf{R}^{-1} \mathbf{Q}^T \mathbf{b} \tag{18} \\
\end{align*}
$$

## Singular Value Decomposition

**Singular Value Decomposition** (Ph√¢n t√≠ch Suy bi·∫øn) l√† m·ªôt ph∆∞∆°ng ph√°p ph√¢n r√£ m·ªôt ma tr·∫≠n b·∫•t k√¨ th√†nh t√≠ch c·ªßa ba ma tr·∫≠n $\mathbf{U}$, $\mathbf{\Sigma}$ v√† $\mathbf{V}^T$, trong ƒë√≥ $\mathbf{U}$ v√† $\mathbf{V}$ l√† c√°c **Orthonormal Matrix** (Ma tr·∫≠n Tr·ª±c chu·∫©n) v√† $\mathbf{\Sigma}$ l√† m·ªôt **Diagonal Matrix** (Ma tr·∫≠n ƒê∆∞·ªùng ch√©o).

### Gi·∫£i b√†i to√°n Least Squares b·∫±ng Singular Value Decomposition

Quay l·∫°i $(7)$, ta c√≥ th·ªÉ ph√¢n r√£ $\mathbf{A}$ th√†nh t√≠ch c·ªßa ba ma tr·∫≠n $\mathbf{U}$, $\mathbf{\Sigma}$ v√† $\mathbf{V}^T$, khi ƒë√≥ $(7)$ tr·ªü th√†nh:

$$
\begin{align*}
\mathbf{A} \mathbf{x} &= \mathbf{b} \\
\mathbf{U} \mathbf{\Sigma} \mathbf{V}^T \mathbf{x} &= \mathbf{b} \\
\mathbf{\Sigma} \mathbf{V}^T \mathbf{x} &= \mathbf{U}^T \mathbf{b} \\
\mathbf{V}^T \mathbf{x} &= \mathbf{\Sigma}^{-1} \mathbf{U}^T \mathbf{b} \\
\mathbf{x} &= \mathbf{V} \mathbf{\Sigma}^{-1} \mathbf{U}^T \mathbf{b} \tag{19} \\
\end{align*}
$$

## Tri·ªÉn khai code Python

To√†n b·ªô code c√≥ th·ªÉ xem chi ti·∫øt t·∫°i: [snowyfield1906/ai-general-research/least_squares
/least_squares.py](https://github.com/SnowyField1906/ai-general-research/blob/main/least_squares/least_squares.py).
